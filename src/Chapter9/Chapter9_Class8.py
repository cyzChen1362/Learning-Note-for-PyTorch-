r"""
学习笔记

原始工程来源：
    D2L (Dive into Deep Learning) 中文版
    仓库地址：https://github.com/d2l-ai/d2l-zh
    官方网站：https://zh.d2l.ai/

原始文献引用：
    @book{zhang2019dive,
        title={Dive into Deep Learning},
        author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
        note={\url{https://zh.d2l.ai/}},
        year={2020}
    }

用途说明：
    本文件基于《动手学深度学习》中文版（d2l-zh）及其代码进行学习与注释，
    仅作个人学习笔记与交流之用，不用于商业用途。

许可协议：
    原工程遵循 Apache-2.0 许可证。
"""


"""
    区域卷积神经网络（R-CNN）系列
"""

# ********************************************************************************
# 这一节全是理论，没有代码，所以这里也仅做理论解释好了
# 原书：https://zh.d2l.ai/chapter_computer-vision/rcnn.html
# ********************************************************************************

# ========================
# R-CNN
# ========================

# ********************************************************************************
# 候选区域生成（Selective Search）：
#   使用“选择性搜索”算法：
#   1. 像素分割：将整张图切成几百上千个小区域，每个区域内部颜色、纹理相似
#   2. 区域相似度计算：颜色直方图相似、纹理直方图相似、区域大小、邻接性
#   3. 迭代合并：不断选择最相似的一对区域进行合并，计算相似度......直到只剩下一个区域
#   4. 生成候选框：在上述过程中，每个区域都生成一个最小外接矩形作为候选框
#
# 特征提取（CNN 前向计算）：
#   1. 将每个候选框裁剪、缩放到统一尺寸（如 224×224，预训练的大小），
#      分别送入一个预训练好的卷积神经网络（如 AlexNet、VGG）做前向传播。
#   2. CNN 的输出高维特征向量代表该区域的外观信息。
#
# 分类（SVM）：
#   1. 对每个候选区域的特征，用多个二分类 SVM 判断它属于哪一类目标（或背景）。
#      这一步相当于“图中这个框是狗吗？是猫吗？是背景吗？”
#   2. 然后这些候选框会计算它们与真实框的交并比，大于阈值的才认为这个候选框覆盖了某个真实目标
#      如果有多个IoU符合阈值就取最高那个，认为候选框的类别真实值等于该真实框
#   3. 每个候选框经过CNN-->SVM，就会得到类别预测值
#   4. 然后真实值-预测值，得到类别hinge loss，训练SVM参数即可，训练不回传CNN
#
# 边框回归（Bounding Box Regression）：
#   1. 同样，根据上面提到的候选框和对应它的真实框，能通过几何关系算出偏移量真实值
#   2. 同时，用一个线性回归器，CNN-->线性回归得到每个候选框的偏移量预测值
#   3. 所以，偏移量真实值-偏移量预测值，算loss，训练线性回归器参数即可，训练不回传CNN
#
# CNN的参数哪里来？
#   1. 我们都看到了，上面只提到了训练SVM和线性回归器的参数，没提到训练CNN的参数
#   2. 但其实，CNN的参数使用的是微调的方法，首先ImageNet分类任务预训练CNN
#   3. 然后将上面提到的候选框的缩放图塞进CNN，输出预测值为这张图片存在的类别+1
#      当然，真实值就是这个候选框IoU对应最好真实框的类别
#   4. 如果预训练的类别数和微调的类别数不一样？那就更换最后的全连接层好了
#   5. Therefore，预训练+微调，得到CNN的值，后面训练SVM和线性回归器就好了
# ********************************************************************************

# ========================
# Fast R-CNN
# ========================

# ********************************************************************************
# 整图卷积：
#   1. 输入原始整张图像到 CNN（例如 VGG16），得到一张共享的特征图 c*h1*w1
#
# 候选区域映射到特征图：
#   1. 使用“选择性搜索”算法，同样得到许多候选框
#   2. 就像特征图映射到锚框再原图上的坐标一样，原图上的候选框坐标也映射到特征图上
#      对应区域就是所谓的兴趣区域
#
# ROI Pooling（兴趣区域池化层）：
#   1. 对兴趣区域进行裁剪和划分，例如兴趣区域14*14，目标输出7*7，
#      那就将兴趣区域划分为7*7个2*2
#   2. 对每个划分区域（例如2*2）进行max pooling得到7*7最大池化输出
#
# 全连接 + 多任务输出：
#   1. 先把池化后的特征摊平成一维：
#   2. 经过两层全连接（通常称 fc6、fc7），得到固定长度的高层语义向量（如 4096 维）
#   3. ① Softmax 分类器：
#      对于第二步的高层语义向量用全连接输出N+1维，损失交叉熵，如果想换成1*1卷积也行
#      ② 线性回归器：
#      以同样的高层向量为输入，输出大小4×(N)or4×(N+1)，表示每个类别的(Δx,Δy,Δw,Δh)
#      损失函数为边框平滑L1损失
#
# 联合损失函数：
#   1. 总损失 = 分类交叉熵 + λ × 边框平滑 L1 损失
#   2. 端到端一次性反向传播更新 CNN 全部参数
# ********************************************************************************

# ========================
# Faster R-CNN
# ========================

# ********************************************************************************
# 与Fast R-CNN相比，Faster R-CNN只有生成提议区域的方法从选择性搜索变成了区域提议网络
# 其他没有变化
# 可以不仅学会检测，还自动学会如何提出高质量候选区域
#
# 区域提议网络：
#
#   1. 同Fast R-CNN，整图卷积，得到的特征图作为区域提议网络的输入
#      RPN 与后续分类/回归分支共用这张特征图，不用重复卷积
#
#   2. 以每个特征点为锚点生成锚框：
#      在特征图的每个空间位置 (i,j)，放置 k 个不同尺度和长宽比的“锚框”
#      例如3个尺度×3个长宽比=9个锚框
#      这些锚框映射回原图后覆盖各种大小、形状的候选区域
#
#   3. 共享 3×3 卷积滑窗：
#      填充1，在整张特征图上滑动一个 3×3 卷积，输出 c 维特征（常用 512）
#      相当于为每个锚点提取一个局部特征向量，供后续预测使用
#
#   4. 两个并行的 1×1 卷积分支：
#      对每个锚点的 c 维特征，RPN 同时输出两类结果：
#      ① 分类分支（objectness）
#        1×1 卷积，输出2k维（每个点k个锚框，每个锚框前景/背景二分类概率）
#        训练时用二元交叉熵，Lcls=−[ylogp+(1−y)log(1−p)]
#        前景样本，真实标签y=1；背景样本，真实标签y=0，p是预测有前景的概率
#      ② 边框回归分支
#        1×1 卷积，输出4k维（每个锚框的Δx,Δy,Δw,Δh）
#        训练时用 Smooth L1 损失，让预测框更贴近真实目标
#
#   5. 训练样本的匹配规则：
#      计算锚框与真实框的 IoU：
#      IoU ≥ 0.7 → 正样本（相当于第四步的y=1）
#      IoU ≤ 0.3 → 负样本（相当于第四步的y=0）
#      介于其间不参与训练
#
#   6. 生成最终候选区域：
#      对所有锚框的回归结果加上预测偏移，得到一批候选框
#      去除越界框，保留p最高的N个
#      非极大值抑制，去除重叠度高的框，保留部分高质量提议框
#
#   7. 端到端联合训练
#      Faster R-CNN 的总损失 = RPN的(分类+回归) + Fast R-CNN的(分类+回归)
#      共享卷积层参数，整网一次反向传播即可
# ********************************************************************************

# ========================
# Mask R-CNN
# ========================

# ********************************************************************************
# Mask R-CNN是Faster R-CNN的进化版
# 要求数据集上标注了每个目标在图像上的像素级位置
# 具体结构图见https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter09_computer-vision/9.8_rcnn
# 根据相对于Faster R-CNN的改变情况，这里介绍Mask R-CNN里新添加的东西：
#
# ROIAlign：
# 用来平替ROI Pooling，更加精细地输出ROI映射后的特征图的网格划分取值结果
# 1. 对于网格划分，不是粗暴地取整，而是精细地用浮点数坐标
# 2. 划分的网格有浮点数坐标，说明网格的中心也是浮点数坐标
# 3. 根据网格中心的浮点数坐标和它周围有交集的几个特征图的像素的坐标距离，加权取值并相加
# 通过距离加权算出某个网格的取值，比ROIPooling的max取值精细得多
# 详情：https://chatgpt.com/s/t_68d79588575c8191beb89f881865e5e6
#      https://chatgpt.com/s/t_68d795974fc08191af8fa173f2d142f8
# 输出也和ROIPooling一样，ROI特征图调整宽高后的结果
#
# 掩码分支（Mask Head）：
# 对于ROIAlign的输出，一个分支是“全连接 + 多任务输出”，这个和之前一样，另一个就是掩码分支
# 1. ROIAlign的输出经过几个卷积/反卷积，然后通过1*1卷积到K通道，其中K为类别数（不含背景）
# 2. K通道的每个通道的（例如）28*28的图，每个像素都对应着这个ROI的这个像素的这个类别是否为前景
#    然后每个像素都是一个sigmoid二分类输出，前景or背景
# 3. 然后训练也很简单，因为训练集有像素级的分类
# 4. 在“全连接 + 多任务输出”分支中，决定了分类，然后在K通道中找到对应分类的28*28图
#    这个图每个像素的sigmoid就是这个像素在这个类别是否为前景了
# 5. 那么预测的时候，“全连接 + 多任务输出”分支给出这个框的类别和偏移量
#    然后根据类别找全卷积层输出的那一类掩码预测，盖到预测框上就是像素级分类了
#
# 总损失 = 分类交叉熵 + λ × 边框平滑 L1 损失 + RPN的(分类+回归) + 掩码损失，一起训练
# ********************************************************************************

